{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "\n",
    "merged_chunks_df = pd.read_csv('merged_chunks.csv')\n",
    "\n",
    "\n",
    "all_texts = []\n",
    "\n",
    "for col in merged_chunks_df.columns:\n",
    "    \n",
    "    col_texts = merged_chunks_df[col]\n",
    "    for item in col_texts:\n",
    "        if isinstance(item, list):\n",
    "            for subitem in item:\n",
    "                if isinstance(subitem, str) and len(subitem.strip())>0:\n",
    "                    all_texts.append(subitem.strip())\n",
    "        elif isinstance(item, str) and len(item.strip())>0:\n",
    "            all_texts.append(item.strip())\n",
    "\n",
    "# Now we have a Python list of strings in all_texts\n",
    "print(f\"Total number of chunk strings: {len(all_texts)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Build an Index for Retrieval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "import faiss\n",
    "import numpy as np\n",
    "\n",
    "# Load an embedding model. You can choose something more advanced if you want.\n",
    "model_name = \"sentence-transformers/all-MiniLM-L6-v2\"\n",
    "embedder = SentenceTransformer(model_name)\n",
    "\n",
    "# Embed all the chunk texts\n",
    "chunk_embeddings = embedder.encode(all_texts, convert_to_numpy=True, show_progress_bar=True)\n",
    "\n",
    "# Build a Faiss index\n",
    "dimension = chunk_embeddings.shape[1]  # e.g. 384 for MiniLM\n",
    "index = faiss.IndexFlatIP(dimension)   # Cosine similarity if we do normalized vectors\n",
    "# Optionally, normalize embeddings if you want approximate cosine similarity\n",
    "# but let's keep it simple for now.\n",
    "\n",
    "# Add embeddings to the index\n",
    "index.add(chunk_embeddings)\n",
    "print(\"FAISS index built. Number of vectors in the index:\", index.ntotal)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "# For demonstration, let's pick a small generative model (like \"gpt2\").\n",
    "# In real usage, pick a model that can handle longer context (like GPT-Neo, Llama 2, or a local bigger model).\n",
    "gen_model_name = \"gpt2\"\n",
    "gen_tokenizer = AutoTokenizer.from_pretrained(gen_model_name)\n",
    "gen_model = AutoModelForCausalLM.from_pretrained(gen_model_name)\n",
    "\n",
    "# If you have GPU, move model to CUDA\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "gen_model.to(device)\n",
    "\n",
    "def rag_answer(query, top_k=3, max_new_tokens=128):\n",
    "    \"\"\"\n",
    "    1) Embed query\n",
    "    2) Retrieve top-k similar chunks\n",
    "    3) Construct a prompt with retrieved text + query\n",
    "    4) Generate answer\n",
    "    \"\"\"\n",
    "    # 1) Embed\n",
    "    q_embed = embedder.encode([query], convert_to_numpy=True)\n",
    "    \n",
    "    # 2) Retrieve top-k\n",
    "    # (Dot product is bigger = more similar; or if you normalized, do index.search)\n",
    "    D, I = index.search(q_embed, top_k)\n",
    "    \n",
    "    # Gather top-k chunks\n",
    "    relevant_chunks = []\n",
    "    for idx in I[0]:\n",
    "        relevant_chunks.append(all_texts[idx])\n",
    "    \n",
    "    # 3) Construct a prompt\n",
    "    # Below is a naive prompt. In real usage, you might want more refined instructions.\n",
    "    context_text = \"\\n\\n\".join(relevant_chunks)\n",
    "    prompt = (\n",
    "        f\"Here are some relevant pieces of context:\\n\\n\"\n",
    "        f\"{context_text}\\n\\n\"\n",
    "        f\"Answer the question:\\n\\n\"\n",
    "        f\"Question: {query}\\n\\n\"\n",
    "        f\"Answer:\"\n",
    "    )\n",
    "    \n",
    "    # 4) Generate answer\n",
    "    inputs = gen_tokenizer(prompt, return_tensors=\"pt\").to(device)\n",
    "    with torch.no_grad():\n",
    "        outputs = gen_model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=max_new_tokens,\n",
    "            do_sample=False,  # for deterministic greedy generation\n",
    "            top_p=0.9\n",
    "        )\n",
    "    answer_text = gen_tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    \n",
    "    # You may want to strip out the initial prompt\n",
    "    # so let's just return the \"final\" part, i.e. everything after 'Answer:'\n",
    "    if \"Answer:\" in answer_text:\n",
    "        answer_text = answer_text.split(\"Answer:\")[-1].strip()\n",
    "    \n",
    "    return answer_text\n",
    "\n",
    "# Quick test\n",
    "test_query = \"Quels sont les signes cliniques d'une exanth√®me subit ?\"\n",
    "response = rag_answer(test_query, top_k=3, max_new_tokens=64)\n",
    "print(\"User Query:\", test_query)\n",
    "print(\"RAG-based answer:\", response)\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
